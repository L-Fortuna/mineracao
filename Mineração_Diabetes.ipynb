{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**TRABALHO PARA CONCLUSÃO DA DISCIPLINA DE MINERAÇÃO DE DADOS: EXTRAÇÃO DE DADOS COM OBJETIVO DE PREVER DIABETES EM MULHERES**\n",
        "---\n",
        "\n",
        "                                                      LARISSA GEMINIANO FORTUNA\n",
        "                                                      LEONARDO BEARARI SILVA\n",
        "                                                      PROF.DR. MURILO VARGES DA SILVA\n",
        "\n",
        "Base de dados: https://www.kaggle.com/code/rishpande/pima-indians-diabetes-beginner\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Data: Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, DiabetesPedigreeFunction, Age, Outcome\n",
        "\n",
        "Features: 9\n",
        "\n",
        "Instances: 768\n",
        "\n",
        "Missing Values: 45\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Database:\n",
        "---\n",
        "\n",
        "A princípio, foi escolhida a base de dados relacionada anteriormente com objetivo de explorar a previsão de casos de diabetes em mulheres, tendo em vista fatores como idade, gravidez, histórico familiar, entre outros. Dessa forma, ficou definido que a principal investigação é a classificação de mulheres com diabetes.\n",
        "\n",
        "-\n",
        "\n",
        "Dos atributos presentes na base de dados, temos:\n",
        "\n",
        "**Pregnancies** (Número de gravidezes de cada mulher); **Glucose** (Teste de glicose); **Blood Pressure** (Nível de pressão ); **SkinThickness** (Espessura da pele); **Insulin** (Nível de insulina baseado em testes); **BMI** (Íncice de Massa Corpórea); **DiabetesPedigreeFunction** (Histórico familiar - pessoas da famíia com diabetes); **Age** (Idade); **Outcome** (Resultado: 0 para não diabtes / 1 para diabetes).\n",
        "\n"
      ],
      "metadata": {
        "id": "_YWesYLHsXHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "INSTALAÇÕES:\n",
        "\n",
        "pip install ucimlrepo\n",
        "pip install pandas\n",
        "pip install imbalanced-learn\n",
        "pip install matplotlib\n",
        "pip install seaborn\n",
        "pip install pandas-profiling\n"
      ],
      "metadata": {
        "id": "p2EbeWjiE_Yv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CODIGO DO PRÉ-PROCESSAMENTO:"
      ],
      "metadata": {
        "id": "xOfumeaLhR8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Carregar o conjunto de dados\n",
        "caminho_arquivo = r\"C:\\bdmineracao\\diabetes_binary_health_indicators_BRFSS2015.csv\"\n",
        "dados = pd.read_csv(caminho_arquivo)\n",
        "\n",
        "# Exibir os nomes das colunas\n",
        "print(\"Nomes das colunas:\")\n",
        "print(dados.columns)\n",
        "\n",
        "# Identificação e remoção de outliers\n",
        "def remove_outliers(df):\n",
        "    # Calcular a mediana para \"MentHlth\" e \"PhysHlth\"\n",
        "    mediana_mental = df['MentHlth'].median()\n",
        "    mediana_fisica = df['PhysHlth'].median()\n",
        "\n",
        "    # Identificar os outliers com base na mediana\n",
        "    outliers = ((df['MentHlth'] < 0) | (df['MentHlth'] > mediana_mental * 3) | (df['PhysHlth'] < 0) | (df['PhysHlth'] > mediana_fisica * 3))\n",
        "\n",
        "    # Remover as linhas que contêm outliers\n",
        "    df_filtrado = df[~outliers]\n",
        "\n",
        "    return df_filtrado\n",
        "\n",
        "# Remover outliers somente em \"MentHlth\" e \"PhysHlth\"\n",
        "dados_sem_outliers = remove_outliers(dados)\n",
        "\n",
        "# Exibir as primeiras 15 linhas do conjunto de dados sem outliers\n",
        "print(\"\\nPrimeiras 15 linhas do conjunto de dados sem outliers:\")\n",
        "print(dados_sem_outliers.head(25))\n"
      ],
      "metadata": {
        "id": "C4PItSpihYD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CODIGO DE PADRONIZAÇÃO:"
      ],
      "metadata": {
        "id": "-zOwi15Pf1A5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Carregar o conjunto de dados\n",
        "caminho_arquivo = r\"C:\\bdmineracao\\diabetes_binary_health_indicators_BRFSS2015.csv\"\n",
        "dados = pd.read_csv(caminho_arquivo)\n",
        "\n",
        "# Identificação e remoção de outliers\n",
        "def remove_outliers(df):\n",
        "    # Calcular a mediana para \"MentHlth\" e \"PhysHlth\"\n",
        "    mediana_mental = df['MentHlth'].median()\n",
        "    mediana_fisica = df['PhysHlth'].median()\n",
        "\n",
        "    # Identificar os outliers com base na mediana\n",
        "    outliers = ((df['MentHlth'] < 0) | (df['MentHlth'] > mediana_mental * 3) | (df['PhysHlth'] < 0) | (df['PhysHlth'] > mediana_fisica * 3))\n",
        "\n",
        "    # Remover as linhas que contêm outliers\n",
        "    df_filtrado = df[~outliers].copy()  # Cria uma cópia do DataFrame para evitar o aviso\n",
        "\n",
        "    return df_filtrado\n",
        "\n",
        "# Função para normalizar os dados por escala mínima e máxima\n",
        "def normalize_min_max(df):\n",
        "    # Selecionar apenas as colunas numéricas para normalização\n",
        "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "    # Criar um objeto MinMaxScaler\n",
        "    scaler = MinMaxScaler()\n",
        "\n",
        "    # Normalizar os dados apenas nas colunas numéricas\n",
        "    df.loc[:, numeric_cols] = scaler.fit_transform(df.loc[:, numeric_cols])\n",
        "\n",
        "    return df\n",
        "\n",
        "# Remover outliers somente em \"MentHlth\" e \"PhysHlth\"\n",
        "dados_sem_outliers = remove_outliers(dados)\n",
        "\n",
        "# Exibir as primeiras 15 linhas do conjunto de dados sem outliers\n",
        "print(\"\\nPrimeiras 15 linhas do conjunto de dados sem outliers:\")\n",
        "print(dados_sem_outliers.head(15))\n",
        "\n",
        "# Aplicar a normalização por escala mínima e máxima nos dados sem outliers\n",
        "dados_normalizados = normalize_min_max(dados_sem_outliers)\n",
        "\n",
        "# Exibir as primeiras 15 linhas dos dados normalizados\n",
        "print(\"\\nPrimeiras 15 linhas dos dados normalizados:\")\n",
        "print(dados_normalizados.head(15))\n"
      ],
      "metadata": {
        "id": "4KgS0kP1hLJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CODIGO DA NORMALIZAÇÃO:"
      ],
      "metadata": {
        "id": "mnkx0Q65gkhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import warnings\n",
        "\n",
        "# Ignorar warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def principal():\n",
        "    # Atualize os caminhos dos arquivos de entrada e saída\n",
        "    input_file = r'C:\\diabetes\\diabetes.csv'\n",
        "    output_file_abstencao = r'C:\\diabetes\\diabetesClear.csv'\n",
        "    output_file_normalizado = r'C:\\diabetes\\diabetesNormalizado.csv'  # Novo arquivo para dados normalizados\n",
        "\n",
        "    # Defina os nomes das colunas e características\n",
        "    names = ['Número Gestações', 'Glucose', 'pressao Arterial', 'Expessura da Pele', 'Insulina', 'IMC',\n",
        "             'Historico Familiar', 'Idade', 'Resultado']\n",
        "\n",
        "    # Leitura do arquivo CSV, pulando a primeira linha de cada coluna\n",
        "    df = pd.read_csv(input_file, names=names, skiprows=1, na_values='?')\n",
        "\n",
        "\n",
        "    # Cópia do DataFrame original\n",
        "    df_original = df.copy()\n",
        "\n",
        "    # Convertendo colunas para tipos numéricos\n",
        "    for col in df.columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    # Exibição das primeiras 15 linhas do arquivo\n",
        "    print(\"PRIMEIRAS 15 LINHAS\\n\")\n",
        "    print(df.head(15))\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Exibição de informações gerais sobre os dados\n",
        "    print(\"INFORMAÇÕES GERAIS DOS DADOS\\n\")\n",
        "    print(df.info())\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Descrição dos dados\n",
        "    print(\"DESCRIÇÃO DOS DADOS\\n\")\n",
        "    print(df.describe())\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Identificação de colunas com valores faltantes\n",
        "    columns_missing_value = df.columns[df.isnull().any()]\n",
        "    print(\"COLUNAS COM VALORES FALTANTES\\n\")\n",
        "    print(columns_missing_value)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Escolha um método para lidar com valores ausentes (por exemplo, 'mode' para preenchimento com moda)\n",
        "    method = 'median'\n",
        "\n",
        "    for c in columns_missing_value:\n",
        "        UpdateMissingValues(df, c, method)\n",
        "\n",
        "    # Normalização Min-Max\n",
        "    features_to_normalize = ['Número Gestações', 'Glucose', 'pressao Arterial', 'Expessura da Pele', 'Insulina', 'IMC',\n",
        "                             'Idade']\n",
        "    df_normalized = normalize_data(df, features_to_normalize)\n",
        "\n",
        "    # Salvamento do DataFrame pré-processado em um novo arquivo CSV\n",
        "    df.to_csv(output_file_abstencao, index=False)\n",
        "\n",
        "    # Salvamento do DataFrame normalizado em um novo arquivo CSV\n",
        "    df_normalized.to_csv(output_file_normalizado, index=False)\n",
        "\n",
        "\n",
        "def UpdateMissingValues(df, column, method=\"median\", number=0):\n",
        "    if method == 'number':\n",
        "        # Substituindo valores ausentes por um número\n",
        "        df[column].fillna(number, inplace=True)\n",
        "    elif method == 'median':\n",
        "        # Substituindo valores ausentes pela mediana\n",
        "        median = df[column].median()\n",
        "        df[column].fillna(median, inplace=True)\n",
        "    elif method == 'mean':\n",
        "        # Substituindo valores ausentes pela média\n",
        "        mean = df[column].mean()\n",
        "        df[column].fillna(mean, inplace=True)\n",
        "    elif method == 'mode':\n",
        "        # Substituindo valores ausentes pela moda\n",
        "        mode = df[column].mode()[0]\n",
        "        df[column].fillna(mode, inplace=True)\n",
        "\n",
        "\n",
        "def normalize_data(df, features):\n",
        "    scaler = MinMaxScaler()\n",
        "    df_normalized = df.copy()\n",
        "    df_normalized[features] = scaler.fit_transform(df_normalized[features])\n",
        "    return df_normalized\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    principal()\n"
      ],
      "metadata": {
        "id": "NMkbrsGHhqPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CODIGO DO PCA:"
      ],
      "metadata": {
        "id": "ilz3EaWof9vo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "# Função principal\n",
        "def main():\n",
        "    # Caminho para o arquivo de entrada\n",
        "    input_file = r'C:\\diabetes\\diabetesNormalizado.csv'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Lendo o arquivo CSV para um DataFrame\n",
        "    df = pd.read_csv(input_file)\n",
        "    # Converter todas as colunas para numérico, forçando não numéricos a NaN\n",
        "    for column in df.columns[:-1]:  # Ignorando a última coluna 'Resultado'\n",
        "        df[column] = pd.to_numeric(df[column], errors='coerce')\n",
        "    # Exibindo informações sobre o DataFrame original\n",
        "    show_dataframe_info(df, \"DataFrame Original\")\n",
        "\n",
        "    # Selecionando características e alvo\n",
        "    features = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "    target = 'Resultado'\n",
        "    x = df[features].values\n",
        "    y = df[target].values\n",
        "\n",
        "    # Manipulando valores ausentes\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    x_imputed = imputer.fit_transform(x)\n",
        "\n",
        "    # Padronizando as características\n",
        "    scaler = StandardScaler()\n",
        "    x_scaled = scaler.fit_transform(x_imputed)\n",
        "\n",
        "    # PCA 2D\n",
        "    visualize_pca_2d(x_scaled, y)\n",
        "\n",
        "\n",
        "# Função para exibir informações sobre um DataFrame\n",
        "def show_dataframe_info(df, message=\"\"):\n",
        "    print(message + \"\\n\")\n",
        "    print(df.info())  # Informações gerais sobre o DataFrame\n",
        "    print(df.describe())  # Estatísticas descritivas do DataFrame\n",
        "    print(df.head(10))  # Exibindo as primeiras linhas do DataFrame\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Exibição dos valores mínimo e máximo de cada característica\n",
        "    print(\"Valores Mínimos das Características:\")\n",
        "    print(df.min())\n",
        "    print(\"\\nValores Máximos das Características:\")\n",
        "    print(df.max())\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "# Função para visualizar o PCA em 2D\n",
        "def visualize_pca_2d(x, y):\n",
        "    # PCA 2D\n",
        "    pca = PCA(n_components=2)\n",
        "    pca_result = pca.fit_transform(x)\n",
        "\n",
        "    # Plotando os resultados do PCA 2D\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(pca_result[:, 0], pca_result[:, 1], c=y, cmap=plt.cm.get_cmap('viridis', 2), alpha=0.5)\n",
        "    plt.xlabel('Componente Principal 1')\n",
        "    plt.ylabel('Componente Principal 2')\n",
        "    plt.title('PCA 2D')\n",
        "    plt.colorbar(ticks=[0, 1], label='Resultado')\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Chamada da função principal\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "-0X7dow4hykp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CODIGO MATRIZ DE CORELAÇÃO:"
      ],
      "metadata": {
        "id": "FWWS3Bx9gCA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Carregar o conjunto de dados\n",
        "caminho_arquivo = r\"C:\\bdmineracao\\diabetes_012_health_indicators_BRFSS2015.csv\"\n",
        "dados = pd.read_csv(caminho_arquivo)\n",
        "\n",
        "# Selecionar as variáveis relevantes para o PCA\n",
        "variaveis_selecionadas = [\n",
        "    'Diabetes_012', 'HighBP', 'HighChol', 'CholCheck', 'BMI', 'Smoker',\n",
        "    'Stroke', 'HeartDiseaseorAttack', 'PhysActivity', 'Fruits', 'Veggies',\n",
        "    'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth',\n",
        "    'MentHlth', 'PhysHlth', 'DiffWalk', 'Sex', 'Age', 'Education', 'Income'\n",
        "]\n",
        "\n",
        "dados_selecionados = dados[variaveis_selecionadas]\n",
        "\n",
        "# Remover linhas com valores ausentes\n",
        "dados_selecionados.dropna(inplace=True)\n",
        "\n",
        "# Calcular a matriz de correlação\n",
        "matriz_correlacao = dados_selecionados.corr()\n",
        "\n",
        "# Visualizar a matriz de correlação usando um mapa de calor\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(matriz_correlacao, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "plt.title('Matriz de Correlação')\n",
        "plt.show()\n",
        "\n",
        "# Aplicar PCA para redução de dimensionalidade\n",
        "pca = PCA(n_components=2)\n",
        "dados_reduzidos = pca.fit_transform(dados_selecionados.drop(columns=['Diabetes_012']))  # Excluindo a coluna alvo\n",
        "\n",
        "# Criar um DataFrame com os dados reduzidos\n",
        "df_reduzido = pd.DataFrame(data=dados_reduzidos, columns=['PC1', 'PC2'])\n",
        "\n",
        "# Concatenar as colunas 'Diabetes_binary' do DataFrame original com os dados reduzidos\n",
        "df_final = pd.concat([dados[['Diabetes_012']], df_reduzido], axis=1)\n",
        "\n",
        "# Exibir os dados finais com redução de dimensionalidade\n",
        "print(\"\\nDados finais com redução de dimensionalidade:\")\n",
        "print(df_final.head())\n",
        "\n",
        "# Plotar o gráfico de dispersão dos dados reduzidos\n",
        "plt.figure(figsize=(6, 10))\n",
        "plt.scatter(df_final['PC1'], df_final['PC2'], c=df_final['Diabetes_012'], cmap='coolwarm', alpha=0.7)\n",
        "plt.xlabel('Componente Principal 1')\n",
        "plt.ylabel('Componente Principal 2')\n",
        "plt.title('PCA - Diabetes_binary em relação aos Componentes Principais')\n",
        "plt.colorbar(label='Diabetes_binary')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "orCatkdUh2FZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CODIGO DA ANALISE DESCRITIVA:"
      ],
      "metadata": {
        "id": "xbL4WV0qh7WO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Carregar o DataFrame original\n",
        "df = pd.read_csv(r'C:\\diabetes\\diabetesNormalizado.csv')\n",
        "\n",
        "# Converter todas as colunas para numérico, forçando não numéricos a NaN\n",
        "for column in df.columns[:-1]:  # Ignorando a última coluna 'Resultado'\n",
        "    df[column] = pd.to_numeric(df[column], errors='coerce')\n",
        "\n",
        "# Definir as faixas etárias\n",
        "bins = [20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "labels = ['20-29', '30-39', '40-49', '50-59', '60-69', '70-79', '80-89', '90-100']\n",
        "\n",
        "# Adicionar uma coluna 'Faixa Etária' ao DataFrame\n",
        "df['Faixa Etária'] = pd.cut(df['Idade'], bins=bins, labels=labels, right=False)\n",
        "\n",
        "# Agrupar os dados por faixa etária e resultado, contando o número de pessoas em cada grupo\n",
        "result_count = df.groupby(['Faixa Etária', 'Resultado']).size().unstack()\n",
        "\n",
        "# Plotar um gráfico de barras mostrando o número de pessoas em cada faixa etária para cada resultado\n",
        "result_count.plot(kind='bar', stacked=True)\n",
        "plt.xlabel('Faixa Etária')\n",
        "plt.ylabel('Número de Pessoas')\n",
        "plt.title('Número de Pessoas por Faixa Etária e Resultado')\n",
        "plt.legend(title='Resultado')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g6Rs59XHiPfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GMM:"
      ],
      "metadata": {
        "id": "cxerCVDNzvB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Carregar o DataFrame original\n",
        "input_file = r'C:\\diabetes\\diabetesNormalizado.csv'\n",
        "# Defina os nomes das colunas e características\n",
        "names = ['Número Gestações', 'Glucose', 'pressao Arterial', 'Expessura da Pele', 'Insulina', 'IMC',\n",
        "         'Historico Familiar', 'Idade', 'Resultado']\n",
        "features = ['Número Gestações', 'Glucose', 'pressao Arterial', 'Expessura da Pele', 'Insulina', 'IMC',\n",
        "            'Historico Familiar', 'Idade']  # Todas as colunas exceto 'Resultado'\n",
        "target = 'Resultado'\n",
        "\n",
        "# Leitura do arquivo CSV, pulando a primeira linha de cada coluna\n",
        "df = pd.read_csv(input_file, names=names, skiprows=1, na_values='?')\n",
        "\n",
        "# Convertendo colunas para tipos numéricos\n",
        "for col in df.columns:\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "# Remover linhas com valores ausentes (NaN)\n",
        "df = df.dropna()\n",
        "\n",
        "\n",
        "def plot_samples(projected, labels, title):\n",
        "    fig = plt.figure(figsize=(10, 6))\n",
        "    u_labels = np.unique(labels)\n",
        "    for i in u_labels:\n",
        "        plt.scatter(projected[labels == i, 0], projected[labels == i, 1], label=i,\n",
        "                    edgecolor='none', alpha=0.5, cmap=plt.cm.get_cmap('tab10', 10))\n",
        "    plt.xlabel('component 1')\n",
        "    plt.ylabel('component 2')\n",
        "    plt.legend()\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def process_diabetes_data():\n",
        "    # Normalizar os dados\n",
        "    x = df.loc[:, features].values\n",
        "    x = StandardScaler().fit_transform(x)\n",
        "    normalizedDf = pd.DataFrame(data=x, columns=features)\n",
        "    normalizedDf = pd.concat([normalizedDf, df[[target]]], axis=1)\n",
        "\n",
        "    # Aplicar PCA nos dados normalizados\n",
        "    pca = PCA(2)\n",
        "    projected = pca.fit_transform(normalizedDf[features])\n",
        "    print(\"Proporção da variância explicada pelas componentes principais:\", pca.explained_variance_ratio_)\n",
        "    print(\"Dimensão dos dados projetados:\", projected.shape)\n",
        "\n",
        "    return projected, normalizedDf[target].values\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Carregar dataset Digits\n",
        "    digits = load_digits()\n",
        "\n",
        "    # Visualizar dataset Digits\n",
        "    fig = plt.figure(figsize=(6, 6))  # figure size in inches\n",
        "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
        "\n",
        "    for i in range(64):\n",
        "        ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])\n",
        "        ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation='nearest')\n",
        "        ax.text(0, 7, str(digits.target[i]))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # Processar e visualizar dados de diabetes\n",
        "    projected_diabetes, labels_diabetes = process_diabetes_data()\n",
        "\n",
        "    # Aplicando Gaussian Mixture Model (GMM) nos dados de diabetes\n",
        "    gm = GaussianMixture(n_components=2).fit(projected_diabetes)\n",
        "    print(\"Pesos do GMM:\", gm.weights_)\n",
        "    print(\"Médias do GMM:\", gm.means_)\n",
        "    cluster_labels = gm.predict(projected_diabetes)\n",
        "\n",
        "    # Visualizar os resultados do GMM\n",
        "    plot_samples(projected_diabetes, cluster_labels, 'Clusters Labels GMM')\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "ULvkqfhTz0y0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-MEANS:"
      ],
      "metadata": {
        "id": "l5ATeJCpz4SS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "\n",
        "# Defining our kmeans function from scratch\n",
        "def KMeans_scratch(x, k, no_of_iterations):\n",
        "    idx = np.random.choice(len(x), k, replace=False)\n",
        "    # Randomly choosing Centroids\n",
        "    centroids = x[idx, :]  # Step 1\n",
        "\n",
        "    # finding the distance between centroids and all the data points\n",
        "    distances = cdist(x, centroids, 'euclidean')  # Step 2\n",
        "\n",
        "    # Centroid with the minimum Distance\n",
        "    points = np.array([np.argmin(i) for i in distances])  # Step 3\n",
        "\n",
        "    # Repeating the above steps for a defined number of iterations\n",
        "    # Step 4\n",
        "    for _ in range(no_of_iterations):\n",
        "        centroids = []\n",
        "        for idx in range(k):\n",
        "            # Updating Centroids by taking mean of Cluster it belongs to\n",
        "            temp_cent = x[points == idx].mean(axis=0)\n",
        "            centroids.append(temp_cent)\n",
        "\n",
        "        centroids = np.vstack(centroids)  # Updated Centroids\n",
        "\n",
        "        distances = cdist(x, centroids, 'euclidean')\n",
        "        points = np.array([np.argmin(i) for i in distances])\n",
        "\n",
        "    return points\n",
        "\n",
        "\n",
        "def plot_samples(projected, labels, title):\n",
        "    fig = plt.figure()\n",
        "    u_labels = np.unique(labels)\n",
        "    for i in u_labels:\n",
        "        plt.scatter(projected[labels == i, 0], projected[labels == i, 1], label=i,\n",
        "                    edgecolor='none', alpha=0.5, cmap=plt.cm.get_cmap('tab10', 10))\n",
        "    plt.xlabel('component 1')\n",
        "    plt.ylabel('component 2')\n",
        "    plt.legend()\n",
        "    plt.title(title)\n",
        "\n",
        "\n",
        "def main():\n",
        "    input_file = r'C:\\diabetes\\diabetesNormalizado.csv'\n",
        "    # Defina os nomes das colunas e características\n",
        "    names = ['Número Gestações', 'Glucose', 'pressao Arterial', 'Expessura da Pele', 'Insulina', 'IMC',\n",
        "             'Historico Familiar', 'Idade', 'Resultado']\n",
        "    features = ['Número Gestações', 'Glucose', 'pressao Arterial', 'Expessura da Pele', 'Insulina', 'IMC',\n",
        "                'Função Pedigree Diabete', 'Idade', 'Resultado']\n",
        "\n",
        "    # Carregar o DataFrame original, assumindo que a primeira linha é o cabeçalho\n",
        "    df = pd.read_csv(input_file, header=0)\n",
        "    target = 'Resultado'\n",
        "    digits = pd.read_csv(input_file,  # Nome do arquivo com dados\n",
        "                         names=names)  # Nome das colunas\n",
        "    # Selecionar apenas as colunas de características (excluindo a coluna do alvo, 'Resultado')\n",
        "    features = df.iloc[:, :-1]\n",
        "    target = df.iloc[:, -1]\n",
        "\n",
        "    # Normalizar os dados\n",
        "    scaler = StandardScaler()\n",
        "    normalized_features = scaler.fit_transform(features)\n",
        "\n",
        "    # Aplicar PCA para reduzir os dados a 2 componentes principais\n",
        "    pca = PCA(2)\n",
        "    projected = pca.fit_transform(normalized_features)\n",
        "    print(pca.explained_variance_ratio_)\n",
        "    print(projected.shape)\n",
        "\n",
        "    # Visualizar os dados originais após a PCA\n",
        "    plot_samples(projected, target, 'Original Labels')\n",
        "\n",
        "    # Aplicar nossa função KMeans implementada do zero\n",
        "    labels = KMeans_scratch(projected, 2, 5)\n",
        "\n",
        "    # Visualizar os resultados\n",
        "    plot_samples(projected, labels, 'Clusters Labels KMeans from scratch')\n",
        "\n",
        "    # Aplicar a função KMeans do sklearn\n",
        "    kmeans = KMeans(n_clusters=2).fit(projected)\n",
        "    print(kmeans.inertia_)\n",
        "    score = silhouette_score(projected, kmeans.labels_)\n",
        "    print(\"For n_clusters = {}, silhouette score is {})\".format(10, score))\n",
        "\n",
        "    # Visualizar os resultados com o KMeans do sklearn\n",
        "    plot_samples(projected, kmeans.labels_, 'Clusters Labels KMeans from sklearn')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "nzs5qxciz7Hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN:"
      ],
      "metadata": {
        "id": "M3YPn_G5z-Yz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, confusion_matrix\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "def minkowski_distance(a, b, p=1):\n",
        "    dim = len(a)\n",
        "    distance = 0\n",
        "    for d in range(dim):\n",
        "        distance += abs(a[d] - b[d]) ** p\n",
        "    distance = distance ** (1 / p)\n",
        "    return distance\n",
        "\n",
        "\n",
        "def knn_predict(X_train, X_test, y_train, y_test, k, p):\n",
        "    y_hat_test = []\n",
        "    for test_point in X_test:\n",
        "        distances = []\n",
        "        for train_point in X_train:\n",
        "            distance = minkowski_distance(test_point, train_point, p=p)\n",
        "            distances.append(distance)\n",
        "\n",
        "        df_dists = pd.DataFrame(data=distances, columns=['dist'], index=y_train.index)\n",
        "        df_nn = df_dists.sort_values(by=['dist'], axis=0)[:k]\n",
        "        counter = Counter(y_train[df_nn.index])\n",
        "        prediction = counter.most_common()[0][0]\n",
        "        y_hat_test.append(prediction)\n",
        "\n",
        "    return y_hat_test\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "    plt.figure()\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "\n",
        "def main():\n",
        "    input_file = r'C:\\diabetes\\diabetesNormalizado.csv'\n",
        "    df = pd.read_csv(input_file)\n",
        "\n",
        "    # Verificar os nomes das colunas\n",
        "    print(\"Nomes das colunas no DataFrame:\")\n",
        "    print(df.columns)\n",
        "\n",
        "    names = ['Número Gestações', 'Glucose', 'pressao Arterial', 'Expessura da Pele', 'Insulina', 'IMC',\n",
        "             'Historico Familiar', 'Idade', 'Resultado']\n",
        "    features = ['Número Gestações', 'Glucose', 'pressao Arterial', 'Expessura da Pele', 'Insulina', 'IMC',\n",
        "                'Historico Familiar', 'Idade']\n",
        "    target = 'Resultado'\n",
        "\n",
        "    # Verificar se todas as colunas estão presentes\n",
        "    missing_columns = [col for col in features + [target] if col not in df.columns]\n",
        "    if missing_columns:\n",
        "        print(\"As seguintes colunas estão faltando no DataFrame:\", missing_columns)\n",
        "        return\n",
        "\n",
        "    X = df[features]\n",
        "    y = df[target]\n",
        "\n",
        "    print(\"Total samples: {}\".format(X.shape[0]))\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1)\n",
        "    print(\"Total train samples: {}\".format(X_train.shape[0]))\n",
        "    print(\"Total test samples: {}\".format(X_test.shape[0]))\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    y_hat_test = knn_predict(X_train, X_test, y_train, y_test, k=5, p=2)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_hat_test) * 100\n",
        "    f1 = f1_score(y_test, y_hat_test, average='macro')\n",
        "    precision = precision_score(y_test, y_hat_test, average='macro')\n",
        "\n",
        "    print(\"Accuracy K-NN from scratch: {:.2f}%\".format(accuracy))\n",
        "    print(\"F1 Score K-NN from scratch: {:.2f}\".format(f1))\n",
        "    print(\"Precision K-NN from scratch: {:.2f}\".format(precision))\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_hat_test)\n",
        "    plot_confusion_matrix(cm, ['Negative', 'Positive'], False, \"Confusion Matrix - K-NN\")\n",
        "    plot_confusion_matrix(cm, ['Negative', 'Positive'], True, \"Confusion Matrix - K-NN normalized\")\n",
        "\n",
        "    knn = KNeighborsClassifier(n_neighbors=5)\n",
        "    knn.fit(X_train, y_train)\n",
        "    y_hat_test = knn.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_hat_test) * 100\n",
        "    f1 = f1_score(y_test, y_hat_test, average='macro')\n",
        "    precision = precision_score(y_test, y_hat_test, average='macro')\n",
        "\n",
        "    print(\"Accuracy K-NN from sk-learn: {:.2f}%\".format(accuracy))\n",
        "    print(\"F1 Score K-NN from sk-learn: {:.2f}\".format(f1))\n",
        "    print(\"Precision K-NN from sk-learn: {:.2f}\".format(precision))\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_hat_test)\n",
        "    plot_confusion_matrix(cm, ['Negative', 'Positive'], False, \"Confusion Matrix - K-NN sklearn\")\n",
        "    plot_confusion_matrix(cm, ['Negative', 'Positive'], True, \"Confusion Matrix - K-NN sklearn normalized\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "QP3RMWIQ0Htz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN - CROSSVALIDATION:"
      ],
      "metadata": {
        "id": "P2xck41U0JLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, confusion_matrix, make_scorer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "def minkowski_distance(a, b, p=1):\n",
        "    dim = len(a)\n",
        "    distance = 0\n",
        "    for d in range(dim):\n",
        "        distance += abs(a[d] - b[d]) ** p\n",
        "    distance = distance ** (1 / p)\n",
        "    return distance\n",
        "\n",
        "\n",
        "def knn_predict(X_train, X_test, y_train, y_test, k, p):\n",
        "    y_hat_test = []\n",
        "    for test_point in X_test:\n",
        "        distances = []\n",
        "        for train_point in X_train:\n",
        "            distance = minkowski_distance(test_point, train_point, p=p)\n",
        "            distances.append(distance)\n",
        "\n",
        "        df_dists = pd.DataFrame(data=distances, columns=['dist'], index=y_train.index)\n",
        "        df_nn = df_dists.sort_values(by=['dist'], axis=0)[:k]\n",
        "        counter = Counter(y_train[df_nn.index])\n",
        "        prediction = counter.most_common()[0][0]\n",
        "        y_hat_test.append(prediction)\n",
        "\n",
        "    return y_hat_test\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "    plt.figure()\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "\n",
        "def main():\n",
        "    input_file = r'C:\\diabetes\\diabetesNormalizado.csv'\n",
        "    df = pd.read_csv(input_file)\n",
        "\n",
        "    names = ['Número Gestações', 'Glucose', 'pressao Arterial', 'Expessura da Pele', 'Insulina', 'IMC',\n",
        "             'Historico Familiar', 'Idade', 'Resultado']\n",
        "    features = ['Número Gestações', 'Glucose', 'pressao Arterial', 'Expessura da Pele', 'Insulina', 'IMC',\n",
        "                'Historico Familiar', 'Idade']\n",
        "    target = 'Resultado'\n",
        "\n",
        "    X = df[features]\n",
        "    y = df[target]\n",
        "\n",
        "    print(\"Total samples: {}\".format(X.shape[0]))\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1, stratify=y)\n",
        "    print(\"Total train samples: {}\".format(X_train.shape[0]))\n",
        "    print(\"Total test samples: {}\".format(X_test.shape[0]))\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    # Validação cruzada para KNN do zero\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
        "    custom_knn_scores = []\n",
        "    for train_index, test_index in skf.split(X_train, y_train):\n",
        "        X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]\n",
        "        y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
        "\n",
        "        y_hat_test_fold = knn_predict(X_train_fold, X_test_fold, y_train_fold, y_test_fold, k=5, p=2)\n",
        "        accuracy = accuracy_score(y_test_fold, y_hat_test_fold)\n",
        "        custom_knn_scores.append(accuracy)\n",
        "\n",
        "    print(\"Cross-validation Accuracy K-NN from scratch: {:.2f}% (+/- {:.2f}%)\".format(np.mean(custom_knn_scores) * 100,\n",
        "                                                                                      np.std(custom_knn_scores) * 100))\n",
        "\n",
        "    # Validação cruzada para KNN do scikit-learn\n",
        "    knn = KNeighborsClassifier(n_neighbors=5)\n",
        "    cv_scores = cross_val_score(knn, X_train, y_train, cv=skf, scoring='accuracy')\n",
        "\n",
        "    print(\"Cross-validation Accuracy K-NN from scikit-learn: {:.2f}% (+/- {:.2f}%)\".format(np.mean(cv_scores) * 100,\n",
        "                                                                                          np.std(cv_scores) * 100))\n",
        "\n",
        "    # Treinamento e avaliação do modelo completo do scikit-learn\n",
        "    knn.fit(X_train, y_train)\n",
        "    y_hat_test = knn.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_hat_test) * 100\n",
        "    f1 = f1_score(y_test, y_hat_test, average='macro')\n",
        "    precision = precision_score(y_test, y_hat_test, average='macro')\n",
        "\n",
        "    print(\"Accuracy Cross-Validation: {:.2f}%\".format(accuracy))\n",
        "    print(\"F1 Score K-NN Cross-Validation: {:.2f}\".format(f1))\n",
        "    print(\"Precision K-NN Cross-Validation: {:.2f}\".format(precision))\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_hat_test)\n",
        "    plot_confusion_matrix(cm, ['Não Diabetes', 'Diabetes'], False, \"Confusion Matrix - K-NN Cross-Validation\")\n",
        "    plot_confusion_matrix(cm, ['Não Diabetes', 'Diabetes'], True, \"Confusion Matrix - K-NN Cross-Validation Normalized\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "8PSzYZvE0Mdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM:"
      ],
      "metadata": {
        "id": "vsNgMqrD0SI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, confusion_matrix\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "    plt.figure()\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    cm = np.round(cm, 2)\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "\n",
        "def main():\n",
        "    input_file = r'C:\\diabetes\\diabetesNormalizado.csv'\n",
        "    df = pd.read_csv(input_file)\n",
        "\n",
        "    names = ['Número Gestações', 'Glucose', 'pressao Arterial', 'Expessura da Pele', 'Insulina', 'IMC',\n",
        "             'Historico Familiar', 'Idade', 'Resultado']\n",
        "    features = ['Número Gestações', 'Glucose', 'pressao Arterial', 'Expessura da Pele', 'Insulina', 'IMC',\n",
        "                'Historico Familiar', 'Idade']\n",
        "    target = 'Resultado'\n",
        "\n",
        "    X = df[features]\n",
        "    y = df[target]\n",
        "\n",
        "    print(\"Total samples: {}\".format(X.shape[0]))\n",
        "\n",
        "    # Split the data - 75% train, 25% test\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)\n",
        "    print(\"Total train samples: {}\".format(X_train.shape[0]))\n",
        "    print(\"Total test  samples: {}\".format(X_test.shape[0]))\n",
        "\n",
        "    # Scale the X data using Z-score\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    # TESTS USING SVM classifier from sk-learn\n",
        "    svm = SVC(kernel='poly')  # poly, rbf, linear\n",
        "    # training using train dataset\n",
        "    svm.fit(X_train, y_train)\n",
        "    # get support vectors\n",
        "    print(svm.support_vectors_)\n",
        "    # get indices of support vectors\n",
        "    print(svm.support_)\n",
        "    # get number of support vectors for each class\n",
        "    print(\"Qtd Support vectors: \")\n",
        "    print(svm.n_support_)\n",
        "    # predict using test dataset\n",
        "    y_hat_test = svm.predict(X_test)\n",
        "\n",
        "    # Get test accuracy score\n",
        "    accuracy = accuracy_score(y_test, y_hat_test) * 100\n",
        "    f1 = f1_score(y_test, y_hat_test, average='macro')\n",
        "    precision = precision_score(y_test, y_hat_test, average='macro')\n",
        "\n",
        "    print(\"Accuracy SVM: {:.2f}%\".format(accuracy))\n",
        "    print(\"F1 Score SVM: {:.2f}\".format(f1))\n",
        "    print(\"Precision SVM: {:.2f}\".format(precision))\n",
        "\n",
        "    # Get test confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_hat_test)\n",
        "    plot_confusion_matrix(cm, ['Não Diabetes', 'Diabetes'], False, \"Confusion Matrix - SVM \")\n",
        "    plot_confusion_matrix(cm, ['Não Diabetes', 'Diabetes'], True, \"Confusion Matrix - SVM normalized\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "4ZQMU2eL0VL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM - CROSSVALIDATION:"
      ],
      "metadata": {
        "id": "vWAl8P-o0a20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, confusion_matrix\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "    plt.figure()\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    cm = np.round(cm, 2)\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "\n",
        "def main():\n",
        "    input_file = r'C:\\diabetes\\diabetesNormalizado.csv'\n",
        "    df = pd.read_csv(input_file)\n",
        "\n",
        "    names = ['Número Gestações', 'Glucose', 'pressao Arterial', 'Expessura da Pele', 'Insulina', 'IMC',\n",
        "             'Historico Familiar', 'Idade', 'Resultado']\n",
        "    features = ['Número Gestações', 'Glucose', 'pressao Arterial', 'Expessura da Pele', 'Insulina', 'IMC',\n",
        "                'Historico Familiar', 'Idade']\n",
        "    target = 'Resultado'\n",
        "\n",
        "    X = df[features]\n",
        "    y = df[target]\n",
        "\n",
        "    print(\"Total samples: {}\".format(X.shape[0]))\n",
        "\n",
        "    # Split the data - 75% train, 25% test\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)\n",
        "    print(\"Total train samples: {}\".format(X_train.shape[0]))\n",
        "    print(\"Total test  samples: {}\".format(X_test.shape[0]))\n",
        "\n",
        "    # Scale the X data using Z-score\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    # TESTS USING SVM classifier from sk-learn\n",
        "    svm = SVC(kernel='poly')  # poly, rbf, linear\n",
        "\n",
        "    # Perform cross-validation with StratifiedKFold\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
        "    scoring = ['accuracy', 'f1_macro', 'precision_macro']\n",
        "    scores = cross_validate(svm, X_train, y_train, cv=cv, scoring=scoring)\n",
        "\n",
        "    # Extract the cross-validation scores\n",
        "    print(\"Cross-validation scores (Accuracy): \", scores['test_accuracy'])\n",
        "    print(\"Mean cross-validation score (Accuracy): {:.2f}%\".format(np.mean(scores['test_accuracy']) * 100))\n",
        "\n",
        "    print(\"Cross-validation scores (F1 Score): \", scores['test_f1_macro'])\n",
        "    print(\"Mean cross-validation score (F1 Score): {:.2f}\".format(np.mean(scores['test_f1_macro'])))\n",
        "\n",
        "    print(\"Cross-validation scores (Precision): \", scores['test_precision_macro'])\n",
        "    print(\"Mean cross-validation score (Precision): {:.2f}\".format(np.mean(scores['test_precision_macro'])))\n",
        "\n",
        "    # Training using train dataset\n",
        "    svm.fit(X_train, y_train)\n",
        "\n",
        "    # Predict using test dataset\n",
        "    y_hat_test = svm.predict(X_test)\n",
        "\n",
        "    # Get test accuracy score\n",
        "    accuracy = accuracy_score(y_test, y_hat_test) * 100\n",
        "    f1 = f1_score(y_test, y_hat_test, average='macro')\n",
        "    precision = precision_score(y_test, y_hat_test, average='macro')\n",
        "\n",
        "    print(\"Accuracy SVM: {:.2f}%\".format(accuracy))\n",
        "    print(\"F1 Score SVM: {:.2f}\".format(f1))\n",
        "    print(\"Precision SVM: {:.2f}\".format(precision))\n",
        "\n",
        "    # Get test confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_hat_test)\n",
        "    plot_confusion_matrix(cm, ['Não Diabetes', 'Diabetes'], False, \"Confusion Matrix - SVM \")\n",
        "    plot_confusion_matrix(cm, ['Não Diabetes', 'Diabetes'], True, \"Confusion Matrix - SVM normalized\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "Bv0Gzblz0fR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "REDES NEURAIS:"
      ],
      "metadata": {
        "id": "ukCvOvrB0jXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Carregando os dados\n",
        "input_file = r'C:\\diabetes\\diabetesNormalizado.csv'\n",
        "df = pd.read_csv(input_file)\n",
        "\n",
        "# Definindo as colunas\n",
        "names = ['Número Gestações', 'Glucose', 'pressao Arterial', 'Expessura da Pele', 'Insulina', 'IMC',\n",
        "         'Historico Familiar', 'Idade', 'Resultado']\n",
        "features = ['Número Gestações', 'Glucose', 'pressao Arterial', 'Expessura da Pele', 'Insulina', 'IMC',\n",
        "            'Historico Familiar', 'Idade']\n",
        "target = 'Resultado'\n",
        "\n",
        "# Preparando os dados\n",
        "X = df.iloc[:, :-1].values\n",
        "y = df.iloc[:, -1].values\n",
        "\n",
        "# Dividindo os dados em treinamento e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Normalizando os dados\n",
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Treinando a rede neural\n",
        "model = MLPClassifier(hidden_layer_sizes=(10, 10), max_iter=1000, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Fazendo previsões\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculando as métricas\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "\n",
        "# Exibindo as métricas\n",
        "metrics = [['Acurácia', accuracy], ['F1-Score', f1], ['Precisão', precision]]\n",
        "print(tabulate(metrics, headers=['Métrica', 'Valor']))\n",
        "\n",
        "# Plotando a matriz de confusão\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=model.classes_)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title('Matriz de Confusão')\n",
        "plt.show()\n",
        "\n",
        "# Visualizando a estrutura da rede neural (como no seu código)\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "ax.set_title('Rede Neural')\n",
        "ax.set_xlabel('Camada')\n",
        "ax.set_ylabel('Neurônio')\n",
        "ax.grid(True)\n",
        "\n",
        "# Coordenadas dos neurônios\n",
        "neuron_coords = []\n",
        "input_coords = []\n",
        "output_coords = []\n",
        "\n",
        "for i, layer_sizes in enumerate(model.hidden_layer_sizes):\n",
        "    layer_label = f'Camada Oculta {i + 1}'\n",
        "    neurons = layer_sizes\n",
        "    x = i + 0.5\n",
        "\n",
        "    for j in range(neurons):\n",
        "        y = (neurons - 1) / 2 - j\n",
        "        neuron_coords.append((x, y))\n",
        "\n",
        "        if i == 0:\n",
        "            input_coords.append((x - 0.5, y))\n",
        "        elif i == len(model.hidden_layer_sizes) - 1:\n",
        "            output_coords.append((x + 0.5, y))\n",
        "\n",
        "        circle = plt.Circle((x, y), radius=0.1, facecolor='white', edgecolor='black')\n",
        "        ax.add_patch(circle)\n",
        "        ax.annotate(f'Neurônio {j + 1}\\n{layer_label}', (x, y), ha='center', va='center')\n",
        "\n",
        "output_label = 'Camada de Saída'\n",
        "neurons = model.n_outputs_\n",
        "x = len(model.hidden_layer_sizes) + 1.5\n",
        "\n",
        "for j in range(neurons):\n",
        "    y = (neurons - 1) / 2 - j\n",
        "    neuron_coords.append((x, y))\n",
        "    output_coords.append((x, y))\n",
        "\n",
        "    circle = plt.Circle((x, y), radius=0.1, facecolor='white', edgecolor='black')\n",
        "    ax.add_patch(circle)\n",
        "    ax.annotate(f'Neurônio {j + 1}\\n{output_label}', (x, y), ha='center', va='center')\n",
        "\n",
        "for i, (x, y) in enumerate(neuron_coords):\n",
        "    if i < len(neuron_coords) - neurons:\n",
        "        outputs = neuron_coords[i + neurons:]\n",
        "    else:\n",
        "        outputs = output_coords\n",
        "\n",
        "    for output in outputs:\n",
        "        ax.arrow(x, y, output[0] - x, output[1] - y, head_width=0.05, head_length=0.1, fc='black', ec='black')\n",
        "\n",
        "for coord in input_coords:\n",
        "    ax.annotate('Entrada', coord, ha='center', va='center', xytext=(-1, 0), textcoords='offset points')\n",
        "\n",
        "for coord in output_coords:\n",
        "    ax.annotate('Saída', coord, ha='center', va='center', xytext=(1, 0), textcoords='offset points')\n",
        "\n",
        "plt.xlim(-1, len(model.hidden_layer_sizes) + 2)\n",
        "plt.ylim(-(max(neurons for neurons in model.hidden_layer_sizes) - 1) / 2 - 1,\n",
        "         (max(neurons for neurons in model.hidden_layer_sizes) - 1) / 2 + 1)\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F3oWSb-A0luj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "REDES NEURAIS - CROSSVALIDATION"
      ],
      "metadata": {
        "id": "SyiZov2n0rXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
        "import matplotlib.pyplot as plt\n",
        "from tabulate import tabulate\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# Carregar os dados\n",
        "input_file = r'C:\\diabetes\\diabetesNormalizado.csv'\n",
        "df = pd.read_csv(input_file)\n",
        "\n",
        "# Definir as features e o target\n",
        "features = ['Número Gestações', 'Glucose', 'pressao Arterial', 'Expessura da Pele', 'Insulina', 'IMC',\n",
        "            'Historico Familiar', 'Idade']\n",
        "target = 'Resultado'\n",
        "\n",
        "X = df[features].values\n",
        "y = df[target].values\n",
        "\n",
        "# Dividir os dados em treinamento e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Aplicar o Random UnderSampling\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_train_res, y_train_res = rus.fit_resample(X_train, y_train)\n",
        "\n",
        "# Normalizar os dados com StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_res = scaler.fit_transform(X_train_res)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Definir os hiperparâmetros para GridSearch\n",
        "param_grid = {\n",
        "    'hidden_layer_sizes': [(10, 10), (20, 20), (30, 30)],\n",
        "    'activation': ['tanh', 'relu'],\n",
        "    'alpha': [0.0001, 0.001, 0.01],\n",
        "    'learning_rate_init': [0.001, 0.01, 0.1],\n",
        "    'max_iter': [1000, 2000],\n",
        "}\n",
        "\n",
        "# Realizar a busca em grid para encontrar os melhores hiperparâmetros\n",
        "grid_search = GridSearchCV(MLPClassifier(random_state=42, early_stopping=True), param_grid, cv=5, scoring='precision')\n",
        "grid_search.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Melhor combinação de hiperparâmetros\n",
        "best_params = grid_search.best_params_\n",
        "print(f'Melhores Parâmetros: {best_params}')\n",
        "\n",
        "# Treinar o modelo com os melhores hiperparâmetros\n",
        "model = MLPClassifier(**best_params, random_state=42, early_stopping=True)\n",
        "model.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Avaliar o modelo no conjunto de teste\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "\n",
        "metrics = [['Acurácia', accuracy], ['F1-Score', f1], ['Precisão', precision]]\n",
        "print(tabulate(metrics, headers=['Métrica', 'Valor']))\n",
        "\n",
        "# Realizar cross-validation para precisão\n",
        "cv_precision = cross_val_score(model, X_train_res, y_train_res, cv=5, scoring='precision').mean()\n",
        "print(f'Cross-validated Precision: {cv_precision:.4f}')\n",
        "\n",
        "# Plotar a estrutura da rede neural\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "ax.set_title('Rede Neural')\n",
        "ax.set_xlabel('Camada')\n",
        "ax.set_ylabel('Neurônio')\n",
        "ax.grid(True)\n",
        "\n",
        "# Coordenadas dos neurônios\n",
        "neuron_coords = []\n",
        "\n",
        "# Coordenadas das entradas\n",
        "input_coords = []\n",
        "\n",
        "# Coordenadas das saídas\n",
        "output_coords = []\n",
        "\n",
        "# Camadas ocultas\n",
        "for i, layer_sizes in enumerate(model.hidden_layer_sizes):\n",
        "    layer_label = f'Camada Oculta {i + 1}'\n",
        "    neurons = layer_sizes\n",
        "\n",
        "    # Posição da camada oculta no gráfico\n",
        "    x = i + 0.5\n",
        "\n",
        "    for j in range(neurons):\n",
        "        y = (neurons - 1) / 2 - j\n",
        "        neuron_coords.append((x, y))\n",
        "\n",
        "        if i == 0:\n",
        "            input_coords.append((x - 0.5, y))\n",
        "        elif i == len(model.hidden_layer_sizes) - 1:\n",
        "            output_coords.append((x + 0.5, y))\n",
        "\n",
        "        # Plot do neurônio\n",
        "        circle = plt.Circle((x, y), radius=0.1, facecolor='white', edgecolor='black')\n",
        "        ax.add_patch(circle)\n",
        "        ax.annotate(f'Neurônio {j + 1}\\n{layer_label}', (x, y), ha='center', va='center')\n",
        "\n",
        "# Camada de saída\n",
        "output_label = 'Camada de Saída'\n",
        "neurons = model.n_outputs_\n",
        "\n",
        "# Posição da camada de saída no gráfico\n",
        "x = len(model.hidden_layer_sizes) + 1.5\n",
        "\n",
        "for j in range(neurons):\n",
        "    y = (neurons - 1) / 2 - j\n",
        "    neuron_coords.append((x, y))\n",
        "    output_coords.append((x, y))\n",
        "\n",
        "    # Plot do neurônio\n",
        "    circle = plt.Circle((x, y), radius=0.1, facecolor='white', edgecolor='black')\n",
        "    ax.add_patch(circle)\n",
        "    ax.annotate(f'Neurônio {j + 1}\\n{output_label}', (x, y), ha='center', va='center')\n",
        "\n",
        "# Conexões entre neurônios\n",
        "for i, (x, y) in enumerate(neuron_coords):\n",
        "    if i < len(neuron_coords) - neurons:\n",
        "        outputs = neuron_coords[i + neurons:]\n",
        "    else:\n",
        "        outputs = output_coords\n",
        "\n",
        "    for output in outputs:\n",
        "        ax.arrow(x, y, output[0] - x, output[1] - y, head_width=0.05, head_length=0.1, fc='black', ec='black')\n",
        "\n",
        "# Legendas das entradas e saídas\n",
        "for coord in input_coords:\n",
        "    ax.annotate('Entrada', coord, ha='center', va='center', xytext=(-1, 0), textcoords='offset points')\n",
        "\n",
        "for coord in output_coords:\n",
        "    ax.annotate('Saída', coord, ha='center', va='center', xytext=(1, 0), textcoords='offset points')\n",
        "\n",
        "plt.xlim(-1, len(model.hidden_layer_sizes) + 2)\n",
        "plt.ylim(-(max(neurons for neurons in model.hidden_layer_sizes) - 1) / 2 - 1,\n",
        "         (max(neurons for neurons in model.hidden_layer_sizes) - 1) / 2 + 1)\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EapzFVi00xod"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}